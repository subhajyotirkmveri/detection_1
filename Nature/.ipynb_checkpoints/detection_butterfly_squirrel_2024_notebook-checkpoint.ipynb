{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b978a6ef-84e3-4840-a4d9-f4b1eace77a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /home/sysadm/anaconda3/lib/python3.11/site-packages (8.1.37)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (3.8.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.9.0.80)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (2.1.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
      "Requirement already satisfied: filelock in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.4.99)\n",
      "Requirement already satisfied: six>=1.5 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sysadm/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "### Run this cell only once.\n",
    "%pip install ultralytics\n",
    "# !pip install opencv-python-headless\n",
    "# !pip install labelme2yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35c1c7c-30d1-4672-ac4b-ac7dfc5ce463",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f77ac-0d24-42c4-8379-06553588e52b",
   "metadata": {},
   "source": [
    "### LabelMe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bdf2a-fed0-412c-8f9d-18bba95962c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">LabelMe is a renowned, open-source, graphical annotation tool perfect for annotating images and videos. The tool is Python-based and integrates Qt for its graphical interface. With its light nature and simple to use interface, LabelMe is a preferred option for an open-source visual annotation tool.</div><br>\n",
    "\n",
    "<div align=\"justify\">LabelMe provides functionalities to facilitate annotations for object identification, semantic segmentation, and panoptic segmentation for both image and video data. Additionally, it also aids in addressing various computer vision issues like classification and segmentation. With LabelMe, users can create annotations using circles, rectangles (bounding boxes), lines, and polygons.</div><br>\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"justify\">Interested individuals can find the tool on GitHub, and a comprehensive, step-by-step guide for installing LabelMe can be found <a href=\"https://github.com/wkentaro/labelme\">here</a>. If installation seems daunting or impossible due to system restrictions, LabelMe also provides standalone executive applications for Windows, MacOs, and Linux. Simply access the application <a href=\"https://github.com/wkentaro/labelme/releases/tag/v5.0.2\">here</a> and run the executive file to get started.</div><br>\n",
    "\n",
    "<div align=\"justify\">\n",
    "<h4>Drawing Annotations</h4> <ul> <li>Launch LabelMe and open the directory containing the images you aim to annotate. Opening a directory enables batch processing, which hastens the annotation process.</li> <li>Choose the image you wish to annotate from the file list located at the bottom right corner.</li> <li>Begin the annotation by choosing the 'Create Rectangle' option from the Edit menu. Click to set the rectangle starting point on the image and click on the next key point to form a rectangle. Continue this process until you've completed annotating the image, and click the starting keypoint to conclude the annotation.</li> <li>For different annotation formats, select 'Edit' from the title bar and choose the format you prefer.</li> <li>Indicate the annotation by typing in the label or selecting an option from the label list.</li> <li>By default LabelMe saves your annotation as a JSON file in the same folder as the images.For convenience, you can use the automatically-generated filename. For continuous save, select 'Save Automatically' in the File menu at the top left corner.</li> </ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161a44d",
   "metadata": {},
   "source": [
    "#### Naming convention of class label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b3178",
   "metadata": {},
   "source": [
    "<div align=\"justify\"> Please ensure that you follow the following naming convention for the different objects, while labelling. If a different naming convention is followed, it might lead to error during submission on platform. </div>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Object</th>\n",
    "    <th>Naming Convention</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Undamaged Residential Building</td>\n",
    "    <td>undamagedresidentialbuilding</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Damaged Residential Building</td>\n",
    "    <td>damagedresidentialbuilding</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Undamaged Commercial Building</td>\n",
    "    <td>undamagedcommercialbuilding</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Damaged Commercial Building</td>\n",
    "    <td>damagedcommercialbuilding</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b31133-c396-41b3-a4cd-100d5d5a004d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">After the annotation process, participants can move onto the task of building their models. YOLOv8, however, requires annotations to be in a distinct format. Therefore, the next step is to convert these annotations into the YOLOv8-required format. There are numerous utility scripts/packages designed to facilitate the conversion from .json to .txt files, which YOLOv8 requires. In this case, we have utilized the labelme2yolo package to transform the .json file into .txt format and also to generate the configuration file (dataset.yaml)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2190d59-17ad-4810-98b6-77058290bbb5",
   "metadata": {},
   "source": [
    "Here we have annotated around 100 images of which 80 images will be used for training and the rest 20 as test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4a1cd-5190-41a6-96e6-c649ffdb2961",
   "metadata": {},
   "source": [
    "### YOLO Annotations Format and Train Test Split\n",
    "\n",
    "<div align=\"justify\">Since, YOLO algorithms requires the annotations to be in a specific format labels generated using LabelMe annotation tool should be exported to YOLO format with one *.txt file per image. If there are no objects in an image, no *.txt file is required. In case of bounding box annotations the *.txt file should be formatted with one row per object in class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 to 1). If your boxes are in pixels, you should divide x_center and width by image width, and y_center and height by image height. Class numbers should be zero-indexed (start with 0).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa5eee-9167-418d-8d7e-26e76954a71c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">We have annotated 100 images using LabelMe and have converted them to .txt format using the <a href = https://pypi.org/project/labelme2yolo/>LabelMe2Yolo</a> package. We utilized the package to divide the data into training and testing sets, and also to create a .yaml file. By default, labelme2yolo package converts LabelMe annotations to <a href =\"https://docs.ultralytics.com/datasets/obb/#yolo-obb-format\"> YOLO OBB </a> format which has been used to build this model. The YOLO OBB format designates bounding boxes by their four corner points with coordinates normalized between 0 and 1. It follows this format: class_index, x1, y1, x2, y2, x3, y3, x4, y4. Internally, YOLO processes losses and outputs in the xywhr format, which represents the bounding box's center point (xy), width, height, and rotation. In order to get annotations in bounding box format  (x,y,w,h) you need to pass the parameter <i>--output_format bbox</i>. Participants could consider the approach of using standard bounding boxes, without any specific orientation, as a technique for improving their model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4398b-f23b-408f-ab02-1d809c431329",
   "metadata": {},
   "source": [
    "<div align = \"justify\">For more details on how to config the train and test validation split, please refer <a href = https://pypi.org/project/labelme2yolo/>LabelMe2Yolo</a>.By default 80% of the data is considered for Training Data and the rest of the 20% for the test data. Participants can modify this by changing the parameters.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9454e-4f30-497d-a6f8-c5ca642c51b6",
   "metadata": {},
   "source": [
    "#### Generating Train and Test Data along with Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139d151b-277c-46cc-8765-fb6861297af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:labelme2yolo:Converting train set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\u001b[35m 95%\u001b[0m \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting val set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\u001b[35m 86%\u001b[0m \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting test set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !labelme2yolo --json_dir /path/to/labelme_json_dir/\n",
    "!labelme2yolo --json_dir ./Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46f129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:labelme2yolo:Converting train set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m━━\u001b[0m \u001b[35m 95%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting val set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\u001b[35m 86%\u001b[0m \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hINFO:labelme2yolo:Converting test set ...\n",
      "\u001b[2K\u001b[36mConverting...\u001b[0m \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !labelme2yolo --json_dir /path/to/labelme_json_dir/\n",
    "!labelme2yolo --json_dir ./Annotated_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a63bc-2ec3-4269-bc44-a5085ea4b797",
   "metadata": {},
   "source": [
    "## Preparing the File Structure as required by YOLO \n",
    "<div align =\"justify\">The participants are required to construct the file structure as indicated below and adjust the dataset.yaml settings according to the file path provided.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e61604-b18b-4506-8718-445be3bd618a",
   "metadata": {},
   "source": [
    "### File Structure\n",
    "<div align=\"justify\">YOLOv8, being a Deep Learning model for object detection, necessitates a specific directory framework for running effectively. The fundamental structure has a parent folder titled 'datasets', which further contains two subfolders: 'train', and 'val'.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'datasets' folder: As the name indicates, this folder houses the complete data that YOLOv8 will be working on. It is pivotal as the model gains insight, verifies accuracy, and enhances its effectiveness using the data stored here.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'train' subfolder: This is a subset of the datasets folder and houses a collection of both image files (.jpg format) and their corresponding annotation files (.txt format). The training data is used by the model to learn and create a mathematical function that can predict the output.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'val' subfolder: Similarly, this contains the test images and their corresponding annotations. The test data is not learned by the model. Instead, it's used to evaluate the performance of the model's learning.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'dataset.yaml' file: This is a vital document that encompasses critical details concerning the dataset and the manners of their use. It contains paths for training, testing, and their respective numbers of images, the total count of classes, and class labels. This is generated in the previous section - <b>YOLO Annotations Format and Train Test Split</b></div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb07eb-b917-479a-90e0-7b93cb2707a5",
   "metadata": {},
   "source": [
    "The files need to be organised in the following manner:\n",
    "<ul>\n",
    "    <li> dataset.yaml </li>\n",
    "    <li>datasets</li>\n",
    "<ul>\n",
    "    <li> train </li>\n",
    "    <ul>\n",
    "        <li>image1.jpg</li>\n",
    "        <li>image1.txt</li>\n",
    "        <li>image2.jpg</li>\n",
    "        <li>image2.txt</li>\n",
    "        <li>image3.jpg</li>\n",
    "        <li>image3.txt</li>\n",
    "    </ul>\n",
    "    <li> val</li>\n",
    "    <ul>\n",
    "        <li>image4.jpg</li>\n",
    "        <li>image4.txt</li>\n",
    "        <li>image5.jpg</li>\n",
    "        <li>image5.txt</li>\n",
    "        <li>image6.jpg</li>\n",
    "        <li>image6.txt</li>\n",
    "    </ul>\n",
    " \n",
    "</ul>\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123682e-8ef8-45da-8130-9450b6601efe",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "<div align=\"justify\">\n",
    "\n",
    "Once the file structure is set up, we can proceed to the exercise of building the model. This process is iterative in nature, as participants have to keep experimenting and tweaking our model’s settings to improve its performance. Throughout this point, we continually assess the performance of our model using metric like mean average precission (mAP).</div>\n",
    "\n",
    "<div align=\"justify\">Post-training, the model needs to be validated using a different dataset to further confirm that it works accurately when exposed to unseen data. Lastly, our model should also ensure it is not overfit or underfit. An overfit model implies it is too complex and may not work well with new data, while an underfit model implies the model misses the relevant relations between features and target outputs.</div><br>\n",
    "\n",
    "<div align=\"justify\">Once we complete these steps, we will have a working model that we can now use for various predictions and interpretations.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bde60-6841-4742-b94a-834e3bfc800e",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4970ce6-a6ed-47f3-9c6b-f5d19d520030",
   "metadata": {},
   "source": [
    "<div align =\"justify\">Now that we have the data in a format appropriate for developing the model, we can begin training a model. In this demonstration notebook, we have used pre-trained object detection model from YOLOv8 developed by Ultralytics. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customization capabilities.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae54052-eefe-46b5-a1ed-d446a0b1e46f",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 4</strong></h4>\n",
    "<div align =\"justify\">Ultralytics offers different object detection models which vary in size and the number of parameters it learns from the images. Participants can try the various models offered by Ultralytics to improve the accuracy of detection. For details regrading the different object detection models offered by Ultralytics please refer <a href = \"https://docs.ultralytics.com/models/yolov8/\">here.</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ff816-d963-4891-902a-7829f5e656dd",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 5</strong></h4>\n",
    "<div align=\"justify\">Here, we are using YOLOv8 (yolov8n.pt) to build a custom object detection model. Participants can explore other object detection models or come up with a totally different approach inorder to identify the damaged and undamaged buildings. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab008c6-ef91-46af-89a0-9d6b7f4bda17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the model\n",
    "model = YOLO('yolov8n.pt')\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69a3a70-bc5e-4c83-91ee-440ffe4b90bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.1.34 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.2.0+cu121 CPU (AMD Ryzen 7 3700X 8-Core Processor)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./dataset.yaml, epochs=50, time=None, patience=50, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train24, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train24\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train24', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msadhukhanbidit\u001b[0m (\u001b[33mmacdeep\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sysadm/Documents/Project/wandb/run-20240326_131255-2hzgebdq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/macdeep/YOLOv8/runs/2hzgebdq' target=\"_blank\">train24</a></strong> to <a href='https://wandb.ai/macdeep/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/macdeep/YOLOv8' target=\"_blank\">https://wandb.ai/macdeep/YOLOv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/macdeep/YOLOv8/runs/2hzgebdq' target=\"_blank\">https://wandb.ai/macdeep/YOLOv8/runs/2hzgebdq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/sysadm/Documents/Project/datasets/train.cache... 224 image\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sysadm/Documents/Project/datasets/val.cache... 56 images, 0 \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train24/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train24\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.748      3.972      1.455        130        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427    0.00275      0.109    0.00667     0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.579      2.988      1.223        139        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427     0.0179      0.655      0.127     0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G      1.552      2.416      1.189        223        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.162      0.452      0.142     0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.552       2.24      1.186        172        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.226      0.317      0.197      0.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.505      2.077      1.163        191        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427       0.26      0.309      0.187      0.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.484      2.045      1.189        206        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.169      0.446      0.219      0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G      1.464      2.031      1.181        226        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.181      0.454      0.192      0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.437      2.007      1.156        144        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.198      0.318       0.16     0.0867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G      1.439      1.928      1.175        206        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.173      0.347      0.142      0.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      1.423      1.936      1.163        186        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.209      0.389      0.185      0.104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.423      1.907      1.173        194        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.319      0.384       0.23      0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.418      1.871      1.166        189        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.196      0.397      0.217      0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.393      1.842      1.152        240        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.191      0.529      0.243      0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.417      1.824      1.161        174        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.273      0.445      0.234      0.128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.411      1.836      1.159        192        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.225      0.353      0.243      0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.391      1.808      1.139        216        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.199      0.371      0.222      0.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.352      1.758      1.123        171        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427       0.21       0.34      0.222      0.122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.393      1.831      1.148        198        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.297      0.513      0.281      0.157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      1.355      1.781      1.132        116        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.242      0.457      0.268       0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.345      1.703      1.127        169        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.224      0.437      0.239       0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      1.365      1.692      1.119        266        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427       0.17      0.383      0.185     0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.343      1.662      1.117        187        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.289      0.373      0.264      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.343      1.716      1.129        304        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.236      0.441      0.265      0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.332      1.685      1.118        186        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.231       0.44      0.268      0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G      1.288      1.652       1.09        141        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.238      0.482      0.278       0.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.304      1.639      1.107        205        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.222      0.378      0.269      0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.318      1.573      1.109        120        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.286      0.391      0.258      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.296      1.604      1.096        124        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.268      0.378      0.264       0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.318      1.599      1.108        235        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.248      0.448      0.261      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G      1.293      1.598      1.096        106        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.227      0.411      0.258      0.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      1.307      1.545      1.091        211        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.233      0.408      0.239      0.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      1.303      1.587      1.106        188        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.285      0.462      0.285      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      1.274      1.529      1.091        220        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.309      0.364      0.283      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      1.275       1.51      1.066        179        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.237      0.424       0.25      0.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      1.254      1.451      1.083        163        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.248      0.394      0.265      0.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      1.279      1.498      1.097        153        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.242       0.46       0.25      0.138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G      1.275      1.532      1.081        109        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.198      0.471      0.223      0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      1.226      1.477       1.07        188        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.215      0.448      0.252      0.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G       1.24      1.462      1.068        119        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.219      0.436      0.256      0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G      1.222      1.422      1.053        275        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.216      0.456      0.242      0.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G      1.217      1.492      1.076        136        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.239      0.419      0.253      0.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      1.224      1.458      1.077        108        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.254      0.429      0.259      0.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G      1.217      1.441        1.1        144        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.256      0.449      0.252      0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      1.208      1.408      1.084        122        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.225      0.403      0.244      0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G      1.191      1.373      1.073        143        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.208      0.452       0.24      0.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G      1.192      1.327       1.06        117        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.264      0.373      0.252      0.141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      1.163      1.334      1.058        108        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.278      0.369       0.25      0.139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G      1.179      1.334      1.076        159        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.266      0.339      0.244      0.137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G      1.167      1.323      1.061         65        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.241      0.359      0.238      0.134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G      1.159      1.305      1.063        104        512: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.232      0.371      0.233      0.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.591 hours.\n",
      "Optimizer stripped from runs/detect/train24/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train24/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train24/weights/best.pt...\n",
      "Ultralytics YOLOv8.1.9 🚀 Python-3.10.12 torch-2.2.0+cu121 CPU (AMD Ryzen 7 3700X 8-Core Processor)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         56        427      0.235      0.478      0.278       0.16\n",
      "undamagedresidentialbuilding         56         98      0.235        0.5      0.279      0.138\n",
      "damagedcommercialbuilding         56        155      0.248      0.729      0.303      0.177\n",
      "undamagedcommercialbuilding         56        136      0.314      0.647      0.413      0.254\n",
      "damagedresidentialbuilding         56         38      0.143     0.0357      0.117     0.0717\n",
      "Speed: 1.8ms preprocess, 46.2ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train24\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▂▃▄▅▇████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>lr/pg1</td><td>▂▃▄▅▇████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>lr/pg2</td><td>▂▃▄▅▇████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁▄▄▆▆▆▅▄▇▆▇▇▆▆██▅▇▇██▇▇▇▇██▇▇▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁▄▄▆▆▆▅▄▇▆▇▇▆▆██▅▇███▇█▇▇██▇▇▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>metrics/precision(B)</td><td>▁▁▅▆▅▅▅▅█▅▅▇▅▆█▆▅▇▆▆▆▇▇▆▆▇█▆▆▅▆▆▆▇▇▆▇▇▇▆</td></tr><tr><td>metrics/recall(B)</td><td>▁█▅▄▅▅▄▄▅▅▆▅▄▄▆▅▅▄▅▅▄▅▄▅▅▆▄▅▆▆▅▅▅▅▅▅▄▄▄▆</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>█▆▆▆▅▅▄▄▄▄▄▄▄▃▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/cls_loss</td><td>█▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/dfl_loss</td><td>█▄▃▃▃▃▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val/box_loss</td><td>▁▆▆▆▅▆▇█▅▅▄▄▃▅▄▄▆▃▃▃▃▃▄▄▄▆▃▅▇▆▄▄▅▅▅▄▅▅▅▄</td></tr><tr><td>val/cls_loss</td><td>█▆▅▃▂▂▃▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val/dfl_loss</td><td>▄▇▄▂▅▄▇█▃▃▂▃▃▄▂▃▄▂▂▁▃▃▄▄▃▄▂▄▆▄▄▄▄▄▅▄▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>6e-05</td></tr><tr><td>lr/pg1</td><td>6e-05</td></tr><tr><td>lr/pg2</td><td>6e-05</td></tr><tr><td>metrics/mAP50(B)</td><td>0.27808</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.16025</td></tr><tr><td>metrics/precision(B)</td><td>0.23502</td></tr><tr><td>metrics/recall(B)</td><td>0.47794</td></tr><tr><td>model/GFLOPs</td><td>8.197</td></tr><tr><td>model/parameters</td><td>3011628</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>60.016</td></tr><tr><td>train/box_loss</td><td>1.1586</td></tr><tr><td>train/cls_loss</td><td>1.30541</td></tr><tr><td>train/dfl_loss</td><td>1.06284</td></tr><tr><td>val/box_loss</td><td>1.48075</td></tr><tr><td>val/cls_loss</td><td>1.98335</td></tr><tr><td>val/dfl_loss</td><td>1.25376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train24</strong> at: <a href='https://wandb.ai/macdeep/YOLOv8/runs/2hzgebdq' target=\"_blank\">https://wandb.ai/macdeep/YOLOv8/runs/2hzgebdq</a><br/> View job at <a href='https://wandb.ai/macdeep/YOLOv8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0OTQ5MTIyMw==/version_details/v0' target=\"_blank\">https://wandb.ai/macdeep/YOLOv8/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0OTQ5MTIyMw==/version_details/v0</a><br/>Synced 5 W&B file(s), 22 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240326_131255-2hzgebdq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the model on the dataset for 50 epochs\n",
    "results = model.train(data='./dataset.yaml', epochs=50, imgsz=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787c9f-e2de-4e6e-bb34-10f46390de82",
   "metadata": {},
   "source": [
    "<b>Note: Participants, can find the model weights in runs\\detect\\ ....</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb95b28-1298-42cb-a2a2-64ca4795b385",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "<div align =\"justify\">Now that we have trained our model , all that is left is to evaluate it. For evaluation we will generate the mean average precission report. We will then plot the image results to visualise the results. When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit the dataset they are trained on. To estimate the in-sample and out-of-sample performance, we will see the results using the graph now.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d66c90e-4121-4b4c-a30f-34b9f7a1669b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b1d86db1450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(figsize=(15, 10), dpi=80)\n",
    "# reading the image \n",
    "results = img.imread('runs/detect/train24/results.png')   \n",
    "# displaying the image \n",
    "plt.imshow(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187ded6-a4d6-49d8-8f2a-d1e5adb497be",
   "metadata": {},
   "source": [
    "<div align =\"justify\">From the above results we can see that we achieved an overall MAP-50 of 0.34 and the following MAP-50 on the various classes:\n",
    "<ul>\n",
    "    <li>Undamaged Commercial Building -0.29</li>\n",
    "    <li>Undamaged Residential Building -0.69</li>\n",
    "    <li>Damaged Residential Building - 0.23</li>\n",
    "    <li>Damaged Commercial Building -0.16</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622efaaf-3ca9-4f7a-8a82-77922b883ff4",
   "metadata": {},
   "source": [
    "Let us now look at the confusion matrix, to better analyse the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb3545df-adf7-4d6c-83ac-e1d8152e073e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7b1d7c8dcf10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "figure(figsize=(20,15), dpi=80)  \n",
    "# reading the image \n",
    "cf = img.imread('runs/detect/train24/confusion_matrix.png') \n",
    "# displaying the image \n",
    "plt.imshow(cf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32649f-2d09-43e1-91dd-1f6466618b8d",
   "metadata": {},
   "source": [
    "<div align =\"justify\">From the above, we see that the model is able to achieve overall mAP score of <b>0.34</b>. This is not a very good score, so your goal is to improve this score. Despite its decent performance in identifying undamaged residential buildings, with an mAP score of 0.69, the model struggles when it comes to detecting commercial buildings. Furthermore, the results highlight the inherent challenge in identifying damage in both residential and commercial structures.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8647c-4338-454b-9b6c-2d06bf0bbcce",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 6</strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469eaf-444b-465a-af1a-289edce39690",
   "metadata": {},
   "source": [
    "<div align = \"justify\">Participants have the opportunity to improve their model's performance by focusing on classes where the model's object detection is failing, and providing more labels for instances of that class. This could potentially enhance the mean average precision (mAP) score. Furthermore, creating synthetic data is another strategy participants could adopt to boost their results. This artificial data creation has the potential to enrich their model's learning experience and subsequently escalate its predictive performance. In addition, participants are encouraged to explore various other generative AI image models that have the ability to generate synthetic images. This strategy can enhance the training data quantity, which may subsequently result in improved outcomes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a9f7c-37e7-410b-8884-9108c38b9e4d",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c08438-7517-4b43-baf0-0f9077bd454f",
   "metadata": {},
   "source": [
    "<div align = \"justify\">Once you are happy with your model, you can make a submission. To make a submission, you will need to use your model to make predictions on the images we have provided in the <b>\"Data Description (Submission)\"</b> section and upload the file onto the challenge platform.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb347ca-25e2-43b2-9ddf-a73a34eb5b6d",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Please ensure you do not alter the validation file names (.jpg format), prediction file names (.txt format), as well as do not insert spaces within class names in prediction files. If this is done, the submission process will not generate score on scorecard in platform.</div><br>\n",
    "\n",
    "Here is the format of the prediction of classes, which should be followed in prediction files (.txt format).<br>\n",
    "&lt;class_name&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;\n",
    "\n",
    "For example:\n",
    "\n",
    "undamagedresidentialbuilding 0.8850483894348145 91.77708435058594 9.232025146484375 135.427978515625 61.545249938964844\n",
    "\n",
    "damagedresidentialbuilding 0.39302119612693787 417.7862243652344 72.88447570800781 490.5438537597656 124.75260925292969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892be41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "513ebdc5",
   "metadata": {},
   "source": [
    "### Download submission images from platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da902cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89289c8c-0f6e-4474-8664-a2b76fd46819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model = YOLO('/home/sysadm/Downloads/Nature/runs/detect/train/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308cf17b-5ce8-4f81-b592-de944d0db8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sysadm/Downloads/Nature/result_show/test_52.jpg\n",
      "Making a prediction on  test_52.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_52.jpg: 480x640 3 butterflys, 129.9ms\n",
      "Speed: 2.4ms preprocess, 129.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "1 label saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_5.jpg\n",
      "Making a prediction on  test_5.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_5.jpg: 480x640 2 butterflys, 107.8ms\n",
      "Speed: 1.6ms preprocess, 107.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "2 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_21.jpg\n",
      "Making a prediction on  test_21.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_21.jpg: 384x640 1 butterfly, 98.3ms\n",
      "Speed: 2.5ms preprocess, 98.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "3 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_3.jpg\n",
      "Making a prediction on  test_3.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_3.jpg: 480x640 2 butterflys, 135.5ms\n",
      "Speed: 1.3ms preprocess, 135.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "4 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_51.jpg\n",
      "Making a prediction on  test_51.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_51.jpg: 480x640 1 squirrel, 1 butterfly, 116.3ms\n",
      "Speed: 2.5ms preprocess, 116.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "5 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_50.jpg\n",
      "Making a prediction on  test_50.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_50.jpg: 512x640 2 squirrels, 137.6ms\n",
      "Speed: 13.2ms preprocess, 137.6ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "6 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_20.jpg\n",
      "Making a prediction on  test_20.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_20.jpg: 448x640 5 butterflys, 101.5ms\n",
      "Speed: 1.5ms preprocess, 101.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "7 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_10.jpg\n",
      "Making a prediction on  test_10.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_10.jpg: 480x640 3 butterflys, 104.2ms\n",
      "Speed: 1.3ms preprocess, 104.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "8 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n",
      "/home/sysadm/Downloads/Nature/result_show/test_6.jpg\n",
      "Making a prediction on  test_6.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_6.jpg: 352x640 2 squirrels, 86.0ms\n",
      "Speed: 1.0ms preprocess, 86.0ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
      "9 labels saved to runs/detect/predict/labels\n",
      "Output files generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Decoding according to the .yaml file class names order\n",
    "decoding_of_predictions ={0: 'squirrel', 1: 'butterfly'}\n",
    "\n",
    "directory = '/home/sysadm/Downloads/Nature/result_show'\n",
    "# Directory to store outputs\n",
    "results_directory = '/home/sysadm/Downloads/Nature/result_show_text'\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the current object is a file and ends with .jpeg\n",
    "    if os.path.isfile(os.path.join(directory, filename)) and filename.lower().endswith('.jpg'):\n",
    "        # Perform operations on the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(file_path)\n",
    "        print(\"Making a prediction on \", filename)\n",
    "        results = model.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.25)\n",
    "        \n",
    "        for r in results:\n",
    "            conf_list = r.boxes.conf.numpy().tolist()\n",
    "            clss_list = r.boxes.cls.numpy().tolist()\n",
    "            original_list = clss_list\n",
    "            updated_list = []\n",
    "            for element in original_list:\n",
    "                 updated_list.append(decoding_of_predictions[int(element)])\n",
    "\n",
    "        bounding_boxes = r.boxes.xyxy.numpy()\n",
    "        confidences = conf_list\n",
    "        class_names = updated_list\n",
    "\n",
    "        # Check if bounding boxes, confidences and class names match\n",
    "        if len(bounding_boxes) != len(confidences) or len(bounding_boxes) != len(class_names):\n",
    "            print(\"Error: Number of bounding boxes, confidences, and class names should be the same.\")\n",
    "            continue\n",
    "        text_file_name = os.path.splitext(filename)[0]\n",
    "        # Creating a new .txt file for each image in the submission_directory\n",
    "        with open(os.path.join(results_directory, f\"{text_file_name}.txt\"), \"w\") as file:\n",
    "            for i in range(len(bounding_boxes)):\n",
    "                # Get coordinates of each bounding box\n",
    "                left, top, right, bottom = bounding_boxes[i]\n",
    "                # Write content to file in desired format\n",
    "                file.write(f\"{class_names[i]} {confidences[i]} {left} {top} {right} {bottom}\\n\")\n",
    "        print(\"Output files generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34394329-cbc8-4560-9cc4-764c8ff63ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60618657-954c-4297-b6eb-88b931d32ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c53d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sysadm/Downloads/Nature/result_show/test_52.jpg\n",
      "Making a prediction on  test_52.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_52.jpg: 480x640 3 butterflys, 110.7ms\n",
      "Speed: 1.9ms preprocess, 110.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "1 label saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_52.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_5.jpg\n",
      "Making a prediction on  test_5.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_5.jpg: 480x640 2 butterflys, 105.5ms\n",
      "Speed: 1.8ms preprocess, 105.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "2 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_5.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_21.jpg\n",
      "Making a prediction on  test_21.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_21.jpg: 384x640 1 butterfly, 74.5ms\n",
      "Speed: 2.8ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "3 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_21.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_3.jpg\n",
      "Making a prediction on  test_3.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_3.jpg: 480x640 2 butterflys, 99.4ms\n",
      "Speed: 1.5ms preprocess, 99.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "4 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_3.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_51.jpg\n",
      "Making a prediction on  test_51.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_51.jpg: 480x640 1 squirrel, 1 butterfly, 101.2ms\n",
      "Speed: 1.7ms preprocess, 101.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "5 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_51.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_50.jpg\n",
      "Making a prediction on  test_50.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_50.jpg: 512x640 2 squirrels, 105.4ms\n",
      "Speed: 1.8ms preprocess, 105.4ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "6 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_50.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_20.jpg\n",
      "Making a prediction on  test_20.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_20.jpg: 448x640 5 butterflys, 96.8ms\n",
      "Speed: 1.8ms preprocess, 96.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "7 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_20.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_10.jpg\n",
      "Making a prediction on  test_10.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_10.jpg: 480x640 3 butterflys, 97.4ms\n",
      "Speed: 1.7ms preprocess, 97.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "8 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_10.jpg\n",
      "/home/sysadm/Downloads/Nature/result_show/test_6.jpg\n",
      "Making a prediction on  test_6.jpg\n",
      "\n",
      "image 1/1 /home/sysadm/Downloads/Nature/result_show/test_6.jpg: 352x640 2 squirrels, 82.1ms\n",
      "Speed: 1.3ms preprocess, 82.1ms inference, 0.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "9 labels saved to runs/detect/predict2/labels\n",
      "Output image with bounding boxes saved at: /home/sysadm/Downloads/Nature/result_show_picture/test_6.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('/home/sysadm/Downloads/Nature/runs/detect/train/weights/best.pt')\n",
    "\n",
    "# Decoding according to the .yaml file class names order\n",
    "decoding_of_predictions = {0: 'squirrel', 1: 'butterfly'}\n",
    "\n",
    "directory = '/home/sysadm/Downloads/Nature/result_show'\n",
    "# Directory to store outputs\n",
    "results_directory = '/home/sysadm/Downloads/Nature/result_show_picture'\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the current object is a file and ends with .jpeg\n",
    "    if os.path.isfile(os.path.join(directory, filename)) and filename.lower().endswith('.jpg'):\n",
    "        # Perform operations on the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(file_path)\n",
    "        print(\"Making a prediction on \", filename)\n",
    "        results = model.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.25, save_dir=results_directory)\n",
    "        \n",
    "        for r in results:\n",
    "            conf_list = r.boxes.conf.numpy().tolist()\n",
    "            clss_list = r.boxes.cls.numpy().tolist()\n",
    "            original_list = clss_list\n",
    "            updated_list = []\n",
    "            for element in original_list:\n",
    "                updated_list.append(decoding_of_predictions[int(element)])\n",
    "\n",
    "            bounding_boxes = r.boxes.xyxy.numpy()\n",
    "            confidences = conf_list\n",
    "            class_names = updated_list\n",
    "\n",
    "            # Check if bounding boxes, confidences and class names match\n",
    "            if len(bounding_boxes) != len(confidences) or len(bounding_boxes) != len(class_names):\n",
    "                print(\"Error: Number of bounding boxes, confidences, and class names should be the same.\")\n",
    "                continue\n",
    "            \n",
    "            # Load the image using OpenCV\n",
    "            img = cv2.imread(file_path)\n",
    "            \n",
    "            # Draw bounding boxes on the image\n",
    "            for bbox, conf, cls in zip(bounding_boxes, confidences, class_names):\n",
    "                x1, y1, x2, y2 = bbox.astype(int)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, f'{cls} {conf:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # Save the image with bounding boxes drawn\n",
    "            output_file_path = os.path.join(results_directory, filename)\n",
    "            cv2.imwrite(output_file_path, img)\n",
    "            print(f\"Output image with bounding boxes saved at: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559128b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44bec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
